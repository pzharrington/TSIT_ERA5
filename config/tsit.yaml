base: &base

  # data
  num_data_workers: 16
  # in_channels: [0, 1, 2, 3, 17, 18, 19] # u10, v10, t2m, sp, r500, r850, tcwv
  in_channels: [0, 1 ,2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]
  out_channels: [0] # tp
  train_data_path: '/pscratch/sd/s/shas1693/data/era5/train'
  valid_data_path: '/pscratch/sd/s/shas1693/data/era5/test'
  min_path: '/pscratch/sd/s/shas1693/data/era5/mins.npy'
  max_path: '/pscratch/sd/s/shas1693/data/era5/maxs.npy'
  time_means_path:   '/pscratch/sd/s/shas1693/data/era5/time_means.npy'
  global_means_path: '/pscratch/sd/s/shas1693/data/era5/global_means.npy'
  global_stds_path:  '/pscratch/sd/s/shas1693/data/era5/global_stds.npy'
  precip: '/pscratch/sd/p/pharring/ERA5/precip/total_precipitation'
  precip_time_means: '/pscratch/sd/p/pharring/ERA5/precip/total_precipitation/time_means.npy'
  precip_eps: !!float 1e-5 # epsilon for normalizing precip log(1+tp/eps)
  dt: 1 # timestep length
  n_history: 0 # history input
  crop_size_x: None # crop height of input
  crop_size_y: None # crop width of input
  roll: !!bool False
  conttime: !!bool False
  normalization: 'zscore'
  add_grid: !!bool False
  gridtype: 'linear' # or 'sinusoidal'
  gridalong: 'x' # 'x', 'y', or 'both'
  N_grid_channels: 1

  # AFNO
  afno_validate: !!bool True
  afno_wind_N_channels: 20
  afno_model_precip_path: '/pscratch/sd/s/shas1693/results/era5_wind/paper_precip/0/training_checkpoints/best_ckpt.tar'
  afno_model_wind_path: '/pscratch/sd/s/shas1693/results/era5_wind/paper/1/training_checkpoints/best_ckpt.tar'
  afno_patch_size: 8
  afno_num_blocks: 8

  # model types
  model: 'pix2pix' # which model to use
  norm_G: 'spectralfadebatch3x3' # instance normalization or batch normalization for generator
  norm_D: 'spectralinstance' # instance normalization or batch normalization for discriminator
  norm_S: 'spectralinstance' # instance normalization or batch normalization for style stream
  norm_E: 'spectralinstance' # instance normalization or batch normalization for auxiliary encoder

  # input/output sizes
  batch_size: 1 # input batch size
  img_size: [720, 1440] # w,h of input image
  input_nc: 20 # number of input label classes (aka num input channels)
  output_nc: 1 # number of output image channels

  # for generator
  no_gan_loss: !!bool False
  netG: 'tsit' # selects model architecture for generator (tsit | pix2pixhd)
  ngf0: 64 # number of gen filters in first conv layer (l = 0)
  ngf: 64 # num. gen filters for layer l is 2**l * ngf
  init_type: 'xavier' # network initialization [normal|xavier|kaiming|orthogonal]
  init_variance: !!float 0.02 # variance of the initialization distribution
  z_dim: 256 # dimension of the latent z vector
  alpha: !!float 1. # The parameter that controls the degree of stylization (between 0 and 1)
  no_ss: !!bool True # discard the style stream (not needed for standard SIS)
  downsamp: !!bool True # Start from downsampled content rather than random noise vector.
  num_upsampling_blocks: 7 # number of upsampling blocks (aka spatial scales) in generator
  use_periodic_padding: !!bool True # whether or not to use periodic padding along horizontal direction
  additive_noise: !!bool False # whether or not to use additional additive noise at each G stage

  # VAE setup
  nef: 16 # number of encoder filters in the first conv layer
  use_vae: !!bool False # enable training with an image encoder.

  # for training
  niter: 20 # number of iter at starting learning rate. This is NOT the total #epochs. Totla #epochs is niter + niter_decay
  niter_decay: 20 # number of iters to linearly decay learning rate to zero
  optimizer: 'adam' # optimizer type 
  beta1: !!float 0.5 # momentum term of adam
  beta2: !!float 0.999 # momentum term of adam
  lr: !!float 2E-4 # initial learning rate for adam
  D_steps_per_G: 1 # number of discriminator iterations per generator iterations
  use_spec_loss: !!bool False # if specified, use spectral loss term forG
  lambda_spec: 1.  # spectral loss term weight
  use_l1_loss: !!bool False # if specified, use l1 loss term forG
  lambda_l1: 1.  # l1 loss term weight
  use_ff_loss: !!bool False # not currently working with --amp
  freq_weighting_ffl: !!bool False # whether to give more weight to higher frequencies in FFLoss 
  lat_weighting_ffl: !!bool False # whether to give more weight to mid latitudes in FFLoss
  lambda_ffl: 1. # ffl loss term weight
  pretrained: !!bool False
  pretrained_model_path: '/pscratch/sd/j/jpduncan/tsitprecip/experiments/expts/bs64_l1_only/checkpoints/ckpt.tar'

  # for discriminators
  ndf: 64 # number of discrim filters in first conv layer
  lambda_feat: !!float 0.5 # weight for feature matching loss
  no_ganFeat_loss: !!bool False # if specified, do *not* use discriminator feature matching loss
  gan_mode: 'hinge' # GAN loss function [ls|original|hinge]
  netD: 'multiscale' #  selects model architecture for discriminator multiscale|nlayer
  num_D: 2 # number of discriminators in multiscale-discriminator setup
  n_layers_D: 4 # nubmer of layers in each discriminator
  no_TTUR: !!bool True # if specified, do not use two-timescale update rule training scheme
  lambda_kld: !!float 0.05 # KL divergence term weight
  cat_inp: !!bool False # concat input image to discriminator

  # Logging / Weights & Biases
  entity: 'weatherbenching'
  project: 'ERA5_generative'
  log_to_wandb: !!bool True
  resuming: !!bool False
  log_to_screen: !!bool True
  save_checkpoint: !!bool True
  log_steps_to_screen: !!bool False
  log_every_n_steps: 400

  # debugging
  DEBUG: !!bool False


###################
# non-GAN baselines
###################

bs64_l1_only_sweep:
  <<: *base
  ## train params
  batch_size: 64
  add_grid: !!bool True
  ## arch params
  use_l1_loss: !!bool True
  no_gan_loss: !!bool True
  no_ganFeat_loss: !!bool True
  ## hyperparams
  lambda_l1: !!float 1.
  ## logging

bs64_l1_only:
  <<: *base
  ## train params
  batch_size: 64
  add_grid: !!bool True
  ## arch params
  use_l1_loss: !!bool True
  no_gan_loss: !!bool True
  no_ganFeat_loss: !!bool True
  norm_G: 'spectralfadebatch3x3'
  num_upsampling_blocks: 8
  use_periodic_padding: !!bool True
  ## hyperparams
  lr: 5E-4
  beta1: !!float 0.85
  beta2: !!float 0.95
  ## logging
  resuming: !!bool True

bs64_l1_ffl_sweep:
  <<: *base
  ## train params
  batch_size: 64
  add_grid: !!bool True
  ## arch params
  use_l1_loss: !!bool True
  use_ff_loss: !!bool True
  no_gan_loss: !!bool True
  no_ganFeat_loss: !!bool True
  norm_G: 'fadebatch3x3'
  num_upsampling_blocks: 7
  use_periodic_padding: !!bool False
  ## hyperparams
  ## logging

bs64_l1_ffl:
  <<: *base
  ## train params
  batch_size: 64
  add_grid: !!bool True
  ## arch params
  use_l1_loss: !!bool True
  use_ff_loss: !!bool True
  no_gan_loss: !!bool True
  no_ganFeat_loss: !!bool True
  norm_G: 'fadebatch3x3'
  num_upsampling_blocks: 7
  use_periodic_padding: !!bool False
  ## hyperparams
  lr: 5E-4
  beta1: !!float 0.85
  beta2: !!float 0.95
  lambda_l1: !!float 1.
  lambda_ffl: !!float 1.
  ## logging
  resuming: !!bool True

bs64_ffl_only:
  <<: *base
  ## train params
  batch_size: 64
  add_grid: !!bool True
  ## arch params
  use_l1_loss: !!bool False
  use_ff_loss: !!bool True
  no_gan_loss: !!bool True
  no_ganFeat_loss: !!bool True
  norm_G: 'fadebatch3x3'
  num_upsampling_blocks: 7
  use_periodic_padding: !!bool False
  ## hyperparams
  lr: 5E-4
  beta1: !!float 0.85
  beta2: !!float 0.95
  lambda_ffl: !!float 1.
  ## logging
  resuming: !!bool True


#####
# GAN
#####

bs64_ganFeat_sweep:
  <<: *base
  ## train params
  batch_size: 64
  ## arch params
  norm_G: 'fadebatch3x3'
  ## logging

bs64_ganFeat_l1:
  <<: *base
  ## train params
  batch_size: 64
  ## arch params
  num_D: 4
  ngf0: 64
  num_upsampling_blocks: 8
  cat_inp: !!bool True
  use_l1_loss: !!bool True
  use_periodic_padding: !!bool True
  norm_G: 'fadebatch3x3'
  ## hyperparams
  lr: 3.5E-4
  beta1: 0.75
  beta2: 0.998
  lambda_l1: 0.4
  lambda_feat: 0.12
  ## logging
  resuming: !!bool True

bs64_ganFeat_l1_latgrid:
  <<: *base
  ## train params
  batch_size: 64
  add_grid: !!bool True
  ## arch params
  num_D: 4
  ngf0: 64
  num_upsampling_blocks: 8
  cat_inp: !!bool True
  use_l1_loss: !!bool True
  use_periodic_padding: !!bool False
  norm_G: 'fadebatch3x3'
  ## hyperparams
  lr: 3.5E-4
  beta1: 0.75
  beta2: 0.998
  lambda_l1: 0.4
  lambda_feat: 0.12
  ## logging
  resuming: !!bool True

bs64_ganFeat_l1_pretrained:
  <<: *base
  ## train params
  batch_size: 64
  pretrained: !!bool True
  add_grid: !!bool True
  use_l1_loss: !!bool True
  no_gan_loss: !!bool False
  no_ganFeat_loss: !!bool False
  ## arch params
  use_periodic_padding: !!bool True
  cat_inp: !!bool True
  norm_G: 'spectralfadebatch3x3'
  num_upsampling_blocks: 8
  ngf0: 64
  num_D: 4
  ## hyperparams
  lr: 2.5E-4
  beta1: 0.75
  beta2: 0.998
  lambda_l1: 0.2
  lambda_feat: 0.12
  ## logging
  resuming: !!bool True

bs64_ganFeat_vae:
  <<: *base
  ## train params
  batch_size: 64
  add_grid: !!bool True
  ## arch params
  use_vae: True
  use_l1_loss: !!bool True
  use_periodic_padding: !!bool False
  num_upsampling_blocks: 8
  nef: 16
  z_dim: 256
  num_D: 4
  ngf0: 64
  cat_inp: !!bool True
  norm_G: 'fadebatch3x3'
  ## hyperparams
  lr: 3.5E-4
  beta1: 0.75
  beta2: 0.998
  lambda_l1: 0.4
  lambda_feat: 0.12
  ## logging
  resuming: !!bool True

#######
# debug
#######

debug:
  <<: *base
  DEBUG: !!bool True
  ## train params
  # niter_l1_pretrain: 2
  batch_size: 32
  num_data_workers: 8 # batch size // 4
  add_grid: !!bool True
  pretrained: !!bool True
  ## arch params
  use_vae: False
  num_D: 4
  ngf0: 64
  nef: 16
  z_dim: 256
  num_upsampling_blocks: 8
  cat_inp: !!bool True
  use_l1_loss: !!bool True
  use_ff_loss: !!bool False
  no_gan_loss: !!bool False
  no_ganFeat_loss: !!bool False
  use_periodic_padding: !!bool True
  norm_G: 'spectralfadebatch3x3'
  # norm_G: 'fadebatch3x3'
  ## hyperparams
  lr: 3.5E-4
  beta1: 0.75
  beta2: 0.998
  lambda_l1: 0.4
  lambda_feat: 0.12
  # lambda_ffl: 1.
  ## logging
  log_to_wandb: !!bool True
  log_steps_to_screen: !!bool True
  save_checkpoint: !!bool False
  resuming: !!bool False
